# DataProcessing
These are a few codes to grab or handle some kinds of data form wiki.

## Explanation
- The main package I used to in crawler is `BeautifulSoup` which can analyze the structure of html.
- There are some codes like `delete.py` are to fix some small problems such as repetition of data.
- The main package to extract useful info from the dumps of wiki data is `gensim.corpora.wikicorpus` and the way of getting data in a better format from chinese corpus was designed by [苏剑林](http://spaces.ac.cn/archives/4176/).

## More
- The task is easy so some small useful packages are enough.
- The proxy ips in the code are private and invalid now.
